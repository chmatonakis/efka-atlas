#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
e-EFKA PDF Data Extractor - Enhanced Version
Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï€Î¹Î½Î¬ÎºÏ‰Î½
"""

import streamlit as st
import pandas as pd
import io
import tempfile
import os
import re
from pathlib import Path

# Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î®Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ PDF readers
try:
    import camelot
    CAMELOT_AVAILABLE = True
except ImportError:
    CAMELOT_AVAILABLE = False

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· ÏƒÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(
    page_title="e-EFKA PDF Extractor (Enhanced)",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .info-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
    }
    .warning-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
    }
</style>
""", unsafe_allow_html=True)

def extract_tables_with_pdfplumber(pdf_path):
    """Î•Î¾Î¬Î³ÎµÎ¹ Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ pdfplumber Î¼Îµ Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿"""
    import pdfplumber
    
    all_tables = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            if page_num < 1:  # Î Î±ÏÎ±ÎºÎ¬Î¼Ï€Ï„Î¿Ï…Î¼Îµ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î±
                continue
                
            st.info(f"ğŸ” Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÏƒÎµÎ»Î¯Î´Î±Ï‚ {page_num + 1}...")
            
            # Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ­Ï‚ ÎµÎ¾Î±Î³Ï‰Î³Î®Ï‚
            tables_found = False
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 1: ÎšÎ±Î½Î¿Î½Î¹ÎºÎ® ÎµÎ¾Î±Î³Ï‰Î³Î® Ï€Î¹Î½Î¬ÎºÏ‰Î½
            tables = page.extract_tables()
            st.info(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(tables)} Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ ÎºÎ±Î½Î¿Î½Î¹ÎºÎ® Î¼Î­Î¸Î¿Î´Î¿")
            
            if len(tables) >= 2:
                second_table = tables[1]
                if second_table and len(second_table) > 1:
                    df = pd.DataFrame(second_table[1:], columns=second_table[0])
                    df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                    all_tables.append(df)
                    st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (pdfplumber - ÎºÎ±Î½Î¿Î½Î¹ÎºÎ®)")
                    tables_found = True
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 2: Î•Î¾Î±Î³Ï‰Î³Î® Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚
            if not tables_found:
                try:
                    # Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚
                    tables_alt = page.extract_tables(table_settings={
                        "vertical_strategy": "lines_strict",
                        "horizontal_strategy": "lines_strict",
                        "min_words_vertical": 1,
                        "min_words_horizontal": 1
                    })
                    st.info(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(tables_alt)} Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ strict lines")
                    
                    if len(tables_alt) >= 2:
                        second_table = tables_alt[1]
                        if second_table and len(second_table) > 1:
                            df = pd.DataFrame(second_table[1:], columns=second_table[0])
                            df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                            all_tables.append(df)
                            st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (pdfplumber - strict)")
                            tables_found = True
                except Exception as e:
                    st.warning(f"  - Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 2 Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 3: Î•Î¾Î±Î³Ï‰Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Ï€Î¹Î½Î¬ÎºÏ‰Î½ ÎºÎ±Î¹ ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î¿Ï… Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿Ï…
            if not tables_found:
                try:
                    all_tables_page = page.extract_tables()
                    if all_tables_page:
                        # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿Î½ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Ï€Î¯Î½Î±ÎºÎ± (ÎµÎºÏ„ÏŒÏ‚ Î±Ï€ÏŒ Ï„Î¿Î½ Ï€ÏÏÏ„Î¿)
                        if len(all_tables_page) > 1:
                            largest_table = max(all_tables_page[1:], key=len)
                        else:
                            largest_table = all_tables_page[0]
                        
                        if largest_table and len(largest_table) > 1:
                            df = pd.DataFrame(largest_table[1:], columns=largest_table[0])
                            df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                            all_tables.append(df)
                            st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (pdfplumber - Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿Ï‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚)")
                            tables_found = True
                except Exception as e:
                    st.warning(f"  - Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 3 Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 4: Î•Î¾Î±Î³Ï‰Î³Î® Î¼Îµ text-based approach
            if not tables_found:
                try:
                    # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ ÏŒÎ»Î¿ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎºÎ±Î¹ ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± patterns
                    text = page.extract_text()
                    if text and len(text) > 100:  # Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±ÏÎºÎµÏ„ÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
                        # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Î³ÏÎ±Î¼Î¼Î­Ï‚ Ï€Î¿Ï… Î¼Î¿Î¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ Ï€Î¯Î½Î±ÎºÎ±
                        lines = text.split('\n')
                        table_lines = []
                        for line in lines:
                            # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Îµ Ï€Î¿Î»Î»Î¬ ÎºÎµÎ½Î¬ Î® tabs (Ï€Î¹Î¸Î±Î½Î¬ Ï€Î¯Î½Î±ÎºÎµÏ‚)
                            if line.count(' ') > 5 or '\t' in line:
                                table_lines.append(line)
                        
                        if len(table_lines) > 10:  # Î‘Î½ Î²ÏÎ®ÎºÎ±Î¼Îµ Î±ÏÎºÎµÏ„Î­Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚
                            st.info(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(table_lines)} Î³ÏÎ±Î¼Î¼Î­Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Ï€Î¿Ï… Î¼Î¿Î¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ Ï€Î¯Î½Î±ÎºÎ±")
                            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î±Î½ Î±Ï€Î»ÏŒ DataFrame
                            data = []
                            for line in table_lines:
                                # Î§Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î· Î³ÏÎ±Î¼Î¼Î® ÏƒÎµ ÏƒÏ„Î®Î»ÎµÏ‚
                                columns = [col.strip() for col in line.split() if col.strip()]
                                if len(columns) > 3:  # Î‘Î½ Î­Ï‡ÎµÎ¹ Î±ÏÎºÎµÏ„Î­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
                                    data.append(columns)
                            
                            if data and len(data) > 1:
                                # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ DataFrame
                                max_cols = max(len(row) for row in data)
                                for i, row in enumerate(data):
                                    while len(row) < max_cols:
                                        row.append('')
                                
                                df = pd.DataFrame(data[1:], columns=[f'Î£Ï„Î®Î»Î·_{i+1}' for i in range(max_cols)])
                                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                                all_tables.append(df)
                                st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (pdfplumber - text-based)")
                                tables_found = True
                except Exception as e:
                    st.warning(f"  - Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 4 Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 5: Regex-based pattern matching Î³Î¹Î± e-EFKA Ï€Î¯Î½Î±ÎºÎµÏ‚
            if not tables_found:
                try:
                    text = page.extract_text()
                    if text:
                        # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± patterns Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Ï‰Î½ e-EFKA Ï€Î¹Î½Î¬ÎºÏ‰Î½
                        lines = text.split('\n')
                        
                        # Regex patterns Î³Î¹Î± e-EFKA Î´ÎµÎ´Î¿Î¼Î­Î½Î±
                        date_pattern = r'\d{1,2}/\d{1,2}/\d{4}'  # Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚
                        amount_pattern = r'[\d,]+\.\d{2}'  # Î Î¿ÏƒÎ¬ Î¼Îµ Î´ÎµÎºÎ±Î´Î¹ÎºÎ¬
                        days_pattern = r'\d+'  # Î—Î¼Î­ÏÎµÏ‚
                        
                        table_data = []
                        in_table = False
                        header_found = False
                        
                        for i, line in enumerate(lines):
                            line = line.strip()
                            if not line:
                                continue
                            
                            # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± header Ï„Î¿Ï… Ï€Î¯Î½Î±ÎºÎ±
                            if not header_found and ('Î‘Ï€ÏŒ' in line or 'Î—Î¼Î­ÏÎµÏ‚' in line or 'ÎœÎ¹ÎºÏ„Î­Ï‚' in line):
                                header_found = True
                                in_table = True
                                continue
                            
                            # Î‘Î½ ÎµÎ¯Î¼Î±ÏƒÏ„Îµ Î¼Î­ÏƒÎ± ÏƒÏ„Î¿Î½ Ï€Î¯Î½Î±ÎºÎ±
                            if in_table:
                                # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Î³ÏÎ±Î¼Î¼Î­Ï‚ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ Î® Ï€Î¿ÏƒÎ¬
                                if (re.search(date_pattern, line) or 
                                    re.search(amount_pattern, line) or 
                                    line.count(' ') > 3):
                                    
                                    # Î§Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î· Î³ÏÎ±Î¼Î¼Î® ÏƒÎµ ÏƒÏ„Î®Î»ÎµÏ‚
                                    parts = line.split()
                                    if len(parts) >= 4:  # Î‘Î½ Î­Ï‡ÎµÎ¹ Î±ÏÎºÎµÏ„Î­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
                                        table_data.append(parts)
                                else:
                                    # Î‘Î½ Î´ÎµÎ½ Î²ÏÎ®ÎºÎ±Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± 3 Î³ÏÎ±Î¼Î¼Î­Ï‚ ÏƒÏ…Î½ÎµÏ‡ÏŒÎ¼ÎµÎ½Î±, ÏƒÏ„Î±Î¼Î±Ï„Î¬Î¼Îµ
                                    if len(table_data) > 0 and i > 0:
                                        # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î¿Î¹ ÎµÏ€ÏŒÎ¼ÎµÎ½ÎµÏ‚ 3 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î­Ï‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î±
                                        next_lines = lines[i:i+3]
                                        has_data = False
                                        for next_line in next_lines:
                                            if (re.search(date_pattern, next_line) or 
                                                re.search(amount_pattern, next_line)):
                                                has_data = True
                                                break
                                        
                                        if not has_data:
                                            break
                        
                        if table_data and len(table_data) > 1:
                            st.info(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(table_data)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Îµ regex pattern matching")
                            
                            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ DataFrame
                            max_cols = max(len(row) for row in table_data)
                            for row in table_data:
                                while len(row) < max_cols:
                                    row.append('')
                            
                            # Î ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ Î½Î± Î²ÏÎ¿ÏÎ¼Îµ headers
                            if len(table_data) > 0:
                                # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Î³Î½Ï‰ÏƒÏ„Î¬ headers Î±Ï€ÏŒ e-EFKA
                                headers = ['Î‘Ï€ÏŒ', 'ÎˆÏ‰Ï‚', 'Î—Î¼Î­ÏÎµÏ‚', 'ÎœÎ¹ÎºÏ„Î­Ï‚ Î±Ï€Î¿Î´Î¿Ï‡Î­Ï‚', 'Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î•Î¹ÏƒÏ†Î¿ÏÎ­Ï‚', 'Î‘/Îœ Î•ÏÎ³Î¿Î´ÏŒÏ„Î·']
                                if len(headers) <= max_cols:
                                    df = pd.DataFrame(table_data, columns=headers[:max_cols])
                                else:
                                    df = pd.DataFrame(table_data, columns=[f'Î£Ï„Î®Î»Î·_{i+1}' for i in range(max_cols)])
                                
                                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                                all_tables.append(df)
                                st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (pdfplumber - regex pattern)")
                                tables_found = True
                except Exception as e:
                    st.warning(f"  - Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 5 Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            
            # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 6: OCR-based extraction (Î±Î½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿)
            if not tables_found:
                try:
                    # Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î½Î± ÎµÎ¾Î¬Î³Î¿Ï…Î¼Îµ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ OCR
                    import pytesseract
                    from PIL import Image
                    import io
                    
                    # ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ Ï„Î· ÏƒÎµÎ»Î¯Î´Î± ÏƒÎµ ÎµÎ¹ÎºÏŒÎ½Î±
                    pix = page.get_pixmap()
                    img_data = pix.tobytes("png")
                    img = Image.open(io.BytesIO(img_data))
                    
                    # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î¼Îµ OCR
                    ocr_text = pytesseract.image_to_string(img, lang='ell+eng')
                    
                    if ocr_text and len(ocr_text) > 100:
                        st.info(f"  - OCR ÎµÎ¾Î®Î³Î±Î³Îµ {len(ocr_text)} Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚")
                        
                        # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚ ÏƒÏ„Î¿ OCR ÎºÎµÎ¯Î¼ÎµÎ½Î¿
                        lines = ocr_text.split('\n')
                        table_lines = []
                        
                        for line in lines:
                            line = line.strip()
                            if (re.search(r'\d{1,2}/\d{1,2}/\d{4}', line) or  # Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚
                                re.search(r'[\d,]+\.\d{2}', line) or  # Î Î¿ÏƒÎ¬
                                line.count(' ') > 3):  # Î Î¿Î»Î»Î¬ ÎºÎµÎ½Î¬
                                table_lines.append(line)
                        
                        if len(table_lines) > 5:
                            st.info(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(table_lines)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Îµ OCR")
                            
                            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ DataFrame
                            data = []
                            for line in table_lines:
                                parts = line.split()
                                if len(parts) >= 4:
                                    data.append(parts)
                            
                            if data and len(data) > 1:
                                max_cols = max(len(row) for row in data)
                                for row in data:
                                    while len(row) < max_cols:
                                        row.append('')
                                
                                df = pd.DataFrame(data, columns=[f'Î£Ï„Î®Î»Î·_{i+1}' for i in range(max_cols)])
                                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                                all_tables.append(df)
                                st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (OCR)")
                                tables_found = True
                except ImportError:
                    st.info(f"  - OCR Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿ (pytesseract)")
                except Exception as e:
                    st.warning(f"  - Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® 6 Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            
            if not tables_found:
                st.warning(f"âš ï¸ Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï€Î¯Î½Î±ÎºÎ±Ï‚ Î¼Îµ ÎºÎ±Î¼Î¯Î± ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ®")
    
    return all_tables

def extract_tables_with_pymupdf(pdf_path):
    """Î•Î¾Î¬Î³ÎµÎ¹ Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ PyMuPDF"""
    import fitz
    
    doc = fitz.open(pdf_path)
    all_tables = []
    
    for page_num in range(len(doc)):
        if page_num < 1:  # Î Î±ÏÎ±ÎºÎ¬Î¼Ï€Ï„Î¿Ï…Î¼Îµ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î±
            continue
            
        page = doc[page_num]
        
        # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î±Ï€ÏŒ Ï„Î· ÏƒÎµÎ»Î¯Î´Î±
        tables = page.find_tables()
        
        if len(tables) >= 2:
            # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿Î½ Î´ÎµÏÏ„ÎµÏÎ¿ Ï€Î¯Î½Î±ÎºÎ± (index 1)
            second_table = tables[1]
            table_data = second_table.extract()
            
            if table_data and len(table_data) > 1:
                # ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ ÏƒÎµ DataFrame
                df = pd.DataFrame(table_data[1:], columns=table_data[0])
                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                all_tables.append(df)
                
                st.success(f"Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (PyMuPDF)")
            else:
                st.warning(f"Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î”ÎµÏÏ„ÎµÏÎ¿Ï‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚ ÎºÎµÎ½ÏŒÏ‚ (PyMuPDF)")
        else:
            st.warning(f"Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î´ÎµÏÏ„ÎµÏÎ¿Ï‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚ (PyMuPDF)")
    
    doc.close()
    return all_tables

def extract_tables_with_camelot(pdf_path):
    """Î•Î¾Î¬Î³ÎµÎ¹ Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ camelot"""
    try:
        tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')
        
        if not tables:
            return []
        
        # ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿Ï…Ï‚ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î±Î½Î¬ ÏƒÎµÎ»Î¯Î´Î±
        pages_dict = {}
        for table in tables:
            page_num = table.page
            if page_num not in pages_dict:
                pages_dict[page_num] = []
            pages_dict[page_num].append(table)
        
        all_tables = []
        
        # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ ÏƒÎµÎ»Î¯Î´Î± (Î¾ÎµÎºÎ¹Î½ÏÎ½Ï„Î±Ï‚ Î±Ï€ÏŒ ÏƒÎµÎ»Î¯Î´Î± 2)
        for page_num in sorted(pages_dict.keys()):
            if page_num < 2:  # Î Î±ÏÎ±ÎºÎ¬Î¼Ï€Ï„Î¿Ï…Î¼Îµ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î±
                continue
                
            page_tables = pages_dict[page_num]
            
            if len(page_tables) >= 2:
                # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿Î½ Î´ÎµÏÏ„ÎµÏÎ¿ Ï€Î¯Î½Î±ÎºÎ± (index 1)
                second_table = page_tables[1]
                df = second_table.df
                
                # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Î¼Î¹Î± ÏƒÏ„Î®Î»Î· Î¼Îµ Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ ÏƒÎµÎ»Î¯Î´Î±Ï‚
                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num
                
                all_tables.append(df)
                st.success(f"Î£ÎµÎ»Î¯Î´Î± {page_num}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ (camelot)")
            else:
                st.warning(f"Î£ÎµÎ»Î¯Î´Î± {page_num}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î´ÎµÏÏ„ÎµÏÎ¿Ï‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚ (camelot)")
        
        return all_tables
        
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± Î¼Îµ camelot: {str(e)}")
        return []

def extract_tables_with_tabula(pdf_path):
    """Î•Î¾Î¬Î³ÎµÎ¹ Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ tabula-py"""
    try:
        import tabula
        
        # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î±Ï€ÏŒ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÏƒÎµÎ»Î¯Î´ÎµÏ‚ (Î¾ÎµÎºÎ¹Î½ÏÎ½Ï„Î±Ï‚ Î±Ï€ÏŒ ÏƒÎµÎ»Î¯Î´Î± 2)
        tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)
        
        all_tables = []
        
        for i, table in enumerate(tables):
            if table is not None and not table.empty:
                # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ ÏƒÏ„Î®Î»Î· ÏƒÎµÎ»Î¯Î´Î±Ï‚
                table['Î£ÎµÎ»Î¯Î´Î±'] = i + 2  # +2 Î³Î¹Î±Ï„Î¯ Î¾ÎµÎºÎ¹Î½Î¬Î¼Îµ Î±Ï€ÏŒ ÏƒÎµÎ»Î¯Î´Î± 2
                all_tables.append(table)
                st.success(f"Tabula: Î•Î¾Î®Ï‡Î¸Î· Ï€Î¯Î½Î±ÎºÎ±Ï‚ {i+1} Î¼Îµ {len(table)} Î³ÏÎ±Î¼Î¼Î­Ï‚")
        
        return all_tables
        
    except ImportError:
        st.warning("Tabula-py Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿")
        return []
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± Î¼Îµ tabula: {str(e)}")
        return []

def extract_efka_data_enhanced(uploaded_file):
    """
    Î•Î½Î¹ÏƒÏ‡Ï…Î¼Î­Î½Î· ÎµÎ¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î¼Îµ Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚
    """
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    try:
        all_tables = []
        
        # Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Î¼Îµ ÏƒÎµÎ¹ÏÎ¬ Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±Ï‚
        methods = []
        
        if CAMELOT_AVAILABLE:
            methods.append(("camelot", extract_tables_with_camelot))
        
        if PDFPLUMBER_AVAILABLE:
            methods.append(("pdfplumber", extract_tables_with_pdfplumber))
        
        # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ tabula Ï‰Ï‚ ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÎµÏ€Î¹Î»Î¿Î³Î®
        try:
            import tabula
            methods.append(("tabula", extract_tables_with_tabula))
        except ImportError:
            pass
        
        if PYMUPDF_AVAILABLE:
            methods.append(("PyMuPDF", extract_tables_with_pymupdf))
        
        if not methods:
            st.error("âŒ Î”ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ PDF!")
            return pd.DataFrame()
        
        st.info(f"Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚: {', '.join([m[0] for m in methods])}")
        
        # Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ ÎºÎ¬Î¸Îµ Î¼Î­Î¸Î¿Î´Î¿
        for method_name, method_func in methods:
            st.markdown(f"### ğŸ” Î”Î¿ÎºÎ¹Î¼Î® Î¼Îµ {method_name}")
            
            try:
                with st.spinner(f"Î•Î¾Î±Î³Ï‰Î³Î® Î¼Îµ {method_name}..."):
                    tables = method_func(tmp_path)
                
                if tables:
                    all_tables.extend(tables)
                    st.success(f"âœ… {method_name}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(tables)} Ï€Î¯Î½Î±ÎºÎµÏ‚")
                    break  # Î‘Î½ Î²ÏÎ®ÎºÎ±Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î±, ÏƒÏ„Î±Î¼Î±Ï„Î¬Î¼Îµ
                else:
                    st.warning(f"âš ï¸ {method_name}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î¯Î½Î±ÎºÎµÏ‚")
                    
            except Exception as e:
                st.error(f"âŒ {method_name}: Î£Ï†Î¬Î»Î¼Î± - {str(e)}")
                continue
        
        if not all_tables:
            st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ ÎºÎ±Î¼Î¯Î± Î¼Î­Î¸Î¿Î´Î¿")
            return pd.DataFrame()
        
        # Î£Ï…Î½Î´Ï…Î¬Î¶Î¿Ï…Î¼Îµ ÏŒÎ»Î± Ï„Î± DataFrames
        with st.spinner("Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½..."):
            combined_df = pd.concat(all_tables, ignore_index=True)
        
        st.success(f"Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÎµÎ¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(combined_df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
        return combined_df
    
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î®: {str(e)}")
        return pd.DataFrame()
    
    finally:
        # Î”Î¹Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ Ï„Î¿ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

def main():
    """ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï„Î·Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚"""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ“„ e-EFKA PDF Data Extractor (Enhanced)</h1>', unsafe_allow_html=True)
    
    # Sidebar Î¼Îµ Î¿Î´Î·Î³Î¯ÎµÏ‚
    with st.sidebar:
        st.markdown("## ğŸ“‹ ÎŸÎ´Î·Î³Î¯ÎµÏ‚")
        st.markdown("""
        1. **Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ PDF Î±ÏÏ‡ÎµÎ¯Î¿** e-EFKA
        2. **Î Î±Ï„Î®ÏƒÏ„Îµ "Î•Î¾Î±Î³Ï‰Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"**
        3. **Î ÏÎ¿Î²Î¬Î»ÎµÏ„Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±** ÏƒÏ„Î¿Î½ Ï€Î¯Î½Î±ÎºÎ±
        4. **ÎšÎ±Ï„ÎµÎ²Î¬ÏƒÏ„Îµ Ï„Î¿ Excel** Î±ÏÏ‡ÎµÎ¯Î¿
        """)
        
        st.markdown("## ğŸ”§ Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Î’Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚")
        if CAMELOT_AVAILABLE:
            st.success("âœ… camelot (ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î³Î¹Î± grid tables)")
        else:
            st.error("âŒ camelot (Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Ghostscript)")
            
        if PDFPLUMBER_AVAILABLE:
            st.success("âœ… pdfplumber (ÎºÎ±Î»ÏŒ Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚)")
        else:
            st.error("âŒ pdfplumber")
            
        if PYMUPDF_AVAILABLE:
            st.success("âœ… PyMuPDF (Î³ÎµÎ½Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·)")
        else:
            st.error("âŒ PyMuPDF")
        
        st.markdown("## âš ï¸ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ¬")
        st.markdown("""
        - **camelot**: ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ grid lines
        - **pdfplumber**: ÎšÎ±Î»ÏŒ Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï‡Ï‰ÏÎ¯Ï‚ grid
        - **PyMuPDF**: Î“ÎµÎ½Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·, Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ Î±ÎºÏÎ¹Î²Î­Ï‚
        - Î•Î¾Î¬Î³ÎµÎ¹ Î±Ï€ÏŒ **ÏƒÎµÎ»Î¯Î´Î± 2** ÎºÎ±Î¹ Î¼ÎµÏ„Î¬
        - Î Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î¿Î½ **Î´ÎµÏÏ„ÎµÏÎ¿ Ï€Î¯Î½Î±ÎºÎ±** Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ ÏƒÎµÎ»Î¯Î´Î±
        """)
    
    # ÎšÏÏÎ¹Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“¤ Î‘Î½Î­Î²Î±ÏƒÎ¼Î± PDF Î‘ÏÏ‡ÎµÎ¯Î¿Ï…")
        
        # File uploader
        uploaded_file = st.file_uploader(
            "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ PDF Î±ÏÏ‡ÎµÎ¯Î¿ e-EFKA",
            type=['pdf'],
            help="Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ Ï„Î¿ PDF Î±ÏÏ‡ÎµÎ¯Î¿ Ï€Î¿Ï… Î¸Î­Î»ÎµÏ„Îµ Î½Î± Î±Î½Î±Î»ÏÏƒÎµÏ„Îµ"
        )
        
        if uploaded_file is not None:
            st.success(f"âœ… Î‘Î½ÎµÎ²Î»Î®Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿: {uploaded_file.name}")
            st.info(f"ğŸ“Š ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {uploaded_file.size:,} bytes")
    
    with col2:
        st.markdown("### âš™ï¸ Î•Î½Î­ÏÎ³ÎµÎ¹ÎµÏ‚")
        
        if uploaded_file is not None:
            if st.button("ğŸš€ Î•Î¾Î±Î³Ï‰Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½", type="primary", use_container_width=True):
                # Î•Î¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
                df = extract_efka_data_enhanced(uploaded_file)
                
                if not df.empty:
                    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ session state
                    st.session_state['extracted_data'] = df
                    st.session_state['filename'] = uploaded_file.name
                    st.rerun()
        else:
            st.info("Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ Ï€ÏÏÏ„Î± Î­Î½Î± PDF Î±ÏÏ‡ÎµÎ¯Î¿")
    
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    if 'extracted_data' in st.session_state and not st.session_state['extracted_data'].empty:
        st.markdown("---")
        st.markdown("### ğŸ“Š Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î•Î¾Î±Î³Ï‰Î³Î®Ï‚")
        
        df = st.session_state['extracted_data']
        
        # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î“ÏÎ±Î¼Î¼Î­Ï‚", len(df))
        with col2:
            st.metric("Î£Ï„Î®Î»ÎµÏ‚", len(df.columns))
        with col3:
            st.metric("Î£ÎµÎ»Î¯Î´ÎµÏ‚", df['Î£ÎµÎ»Î¯Î´Î±'].nunique() if 'Î£ÎµÎ»Î¯Î´Î±' in df.columns else 0)
        with col4:
            st.metric("ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ DataFrame", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
        
        # Î ÏÎ¿Î²Î¿Î»Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        st.markdown("#### ğŸ“‹ Î ÏÎ¿Î²Î¿Î»Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
        
        # Î¦Î¯Î»Ï„ÏÎ±
        if 'Î£ÎµÎ»Î¯Î´Î±' in df.columns:
            pages = sorted(df['Î£ÎµÎ»Î¯Î´Î±'].unique())
            selected_pages = st.multiselect(
                "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ ÏƒÎµÎ»Î¯Î´ÎµÏ‚ Î³Î¹Î± Ï€ÏÎ¿Î²Î¿Î»Î®:",
                options=pages,
                default=pages[:3] if len(pages) > 3 else pages
            )
            
            if selected_pages:
                filtered_df = df[df['Î£ÎµÎ»Î¯Î´Î±'].isin(selected_pages)]
            else:
                filtered_df = df
        else:
            filtered_df = df
        
        # Î Î»Î®Î¸Î¿Ï‚ Î³ÏÎ±Î¼Î¼ÏÎ½ Ï€ÏÎ¿Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
        rows_to_show = st.slider("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î³ÏÎ±Î¼Î¼ÏÎ½ Ï€ÏÎ¿Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·:", 10, min(100, len(filtered_df)), 20)
        
        # Î Î¯Î½Î±ÎºÎ±Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        st.dataframe(
            filtered_df.head(rows_to_show),
            use_container_width=True,
            height=400
        )
        
        # Download button
        st.markdown("#### ğŸ’¾ ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½")
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Excel Î±ÏÏ‡ÎµÎ¯Î¿Ï…
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='e-EFKA_Data', index=False)
        
        output.seek(0)
        
        # Download button
        st.download_button(
            label="ğŸ“¥ ÎšÎ±Ï„ÎµÎ²Î¬ÏƒÏ„Îµ Excel Î±ÏÏ‡ÎµÎ¯Î¿",
            data=output.getvalue(),
            file_name=f"efka_data_{st.session_state.get('filename', 'extracted')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        # Clear button
        if st.button("ğŸ—‘ï¸ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½", use_container_width=True):
            if 'extracted_data' in st.session_state:
                del st.session_state['extracted_data']
            if 'filename' in st.session_state:
                del st.session_state['filename']
            st.rerun()

if __name__ == "__main__":
    main()
