#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
e-EFKA PDF Data Extractor - Specialized Version
Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ· Î³Î¹Î± e-EFKA PDF Î±ÏÏ‡ÎµÎ¯Î± Î¼Îµ custom parsing
"""

import streamlit as st
import pandas as pd
import io
import tempfile
import os
import re
from pathlib import Path

# Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î®Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ PDF readers
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· ÏƒÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(
    page_title="e-EFKA PDF Extractor (Specialized)",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .info-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
    }
    .warning-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
    }
</style>
""", unsafe_allow_html=True)

def extract_efka_tables_specialized(pdf_path):
    """
    Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· ÎµÎ¾Î±Î³Ï‰Î³Î® Ï€Î¹Î½Î¬ÎºÏ‰Î½ Î³Î¹Î± e-EFKA PDF Î±ÏÏ‡ÎµÎ¯Î±
    """
    import pdfplumber
    
    all_tables = []
    
    with pdfplumber.open(pdf_path) as pdf:
        total_pages = len(pdf.pages)
        st.info(f"ğŸ“„ Î£ÏÎ½Î¿Î»Î¿ ÏƒÎµÎ»Î¯Î´Ï‰Î½: {total_pages}")
        
        for page_num, page in enumerate(pdf.pages):
            if page_num < 1:  # Î Î±ÏÎ±ÎºÎ¬Î¼Ï€Ï„Î¿Ï…Î¼Îµ Ï„Î·Î½ Ï€ÏÏÏ„Î· ÏƒÎµÎ»Î¯Î´Î±
                continue
                
            st.markdown(f"### ğŸ” Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î£ÎµÎ»Î¯Î´Î±Ï‚ {page_num + 1}")
            
            # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ ÏŒÎ»Î¿ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î±Ï€ÏŒ Ï„Î· ÏƒÎµÎ»Î¯Î´Î±
            text = page.extract_text()
            
            if not text:
                st.warning(f"âš ï¸ Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿")
                continue
            
            st.info(f"ğŸ“ Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(text)} Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…")
            
            # Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î±Î½Î¬Î»Ï…ÏƒÎ· Î³Î¹Î± e-EFKA Ï€Î¯Î½Î±ÎºÎµÏ‚
            table_data = parse_efka_table_text(text, page_num + 1)
            
            if table_data and len(table_data) > 1:
                # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ DataFrame
                df = pd.DataFrame(table_data[1:], columns=table_data[0])
                df['Î£ÎµÎ»Î¯Î´Î±'] = page_num + 1
                all_tables.append(df)
                st.success(f"âœ… Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î•Î¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
            else:
                st.warning(f"âš ï¸ Î£ÎµÎ»Î¯Î´Î± {page_num + 1}: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¯Î½Î±ÎºÎ±")
    
    return all_tables

def parse_efka_table_text(text, page_num):
    """
    Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î±Î½Î¬Î»Ï…ÏƒÎ· ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î³Î¹Î± e-EFKA Ï€Î¯Î½Î±ÎºÎµÏ‚
    """
    lines = text.split('\n')
    
    # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Ï„Î¿ header Ï„Î¿Ï… Ï€Î¯Î½Î±ÎºÎ±
    header_line = None
    header_index = -1
    
    # Î“Î½Ï‰ÏƒÏ„Î¿Î¯ headers Î±Ï€ÏŒ e-EFKA
    efka_headers = [
        ['Î‘Ï€ÏŒ', 'ÎˆÏ‰Ï‚', 'Î—Î¼Î­ÏÎµÏ‚', 'ÎœÎ¹ÎºÏ„Î­Ï‚ Î±Ï€Î¿Î´Î¿Ï‡Î­Ï‚', 'Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î•Î¹ÏƒÏ†Î¿ÏÎ­Ï‚', 'Î‘/Îœ Î•ÏÎ³Î¿Î´ÏŒÏ„Î·'],
        ['Î‘Î ÎŸ', 'Î•Î©Î£', 'Î—ÎœÎ•Î¡Î•Î£', 'ÎœÎ™ÎšÎ¤Î•Î£ Î‘Î ÎŸÎ”ÎŸÎ§Î•Î£', 'Î£Î¥ÎÎŸÎ›Î™ÎšÎ•Î£ Î•Î™Î£Î¦ÎŸÎ¡Î•Î£', 'Î‘/Îœ Î•Î¡Î“ÎŸÎ”ÎŸÎ¤Î—'],
        ['Î‘Ï€ÏŒ', 'ÎˆÏ‰Ï‚', 'Î—Î¼Î­ÏÎµÏ‚', 'ÎœÎ¹ÎºÏ„Î­Ï‚', 'Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚', 'Î‘/Îœ'],
        ['Î‘Î ÎŸ', 'Î•Î©Î£', 'Î—ÎœÎ•Î¡Î•Î£', 'ÎœÎ™ÎšÎ¤Î•Î£', 'Î£Î¥ÎÎŸÎ›Î™ÎšÎ•Î£', 'Î‘/Îœ']
    ]
    
    # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± header
    for i, line in enumerate(lines):
        line_clean = line.strip()
        if not line_clean:
            continue
            
        # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î· Î³ÏÎ±Î¼Î¼Î® Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ headers
        for header_variant in efka_headers:
            if all(header_word in line_clean for header_word in header_variant):
                header_line = header_variant
                header_index = i
                st.info(f"  ğŸ“‹ Î’ÏÎ­Î¸Î·ÎºÎµ header: {line_clean}")
                break
        
        if header_line:
            break
    
    if not header_line:
        st.warning(f"  âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ header Ï€Î¯Î½Î±ÎºÎ±")
        return None
    
    # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼ÎµÏ„Î¬ Ï„Î¿ header
    table_data = [header_line]
    data_lines = []
    
    for i in range(header_index + 1, len(lines)):
        line = lines[i].strip()
        if not line:
            continue
        
        # Î£Ï„Î±Î¼Î±Ï„Î¬Î¼Îµ Î±Î½ Î²ÏÎ®ÎºÎ±Î¼Îµ Î½Î­Î¿ header Î® Ï„Î¯Ï„Î»Î¿
        if any(word in line.upper() for word in ['Î£Î•Î›Î™Î”Î‘', 'Î£Î¥ÎÎŸÎ›ÎŸ', 'Î£Î¥ÎÎŸÎ›Î™ÎšÎ‘', 'Î£Î•Î›Î™Î”Î•Î£']):
            break
        
        # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î· Î³ÏÎ±Î¼Î¼Î® Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¯Î½Î±ÎºÎ±
        if is_efka_data_line(line):
            data_lines.append(line)
            st.info(f"  ğŸ“Š Î“ÏÎ±Î¼Î¼Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {line[:50]}...")
    
    st.info(f"  ğŸ“ˆ Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(data_lines)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
    
    # ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ ÏƒÏ„Î®Î»ÎµÏ‚
    for line in data_lines:
        row_data = parse_efka_data_line(line, len(header_line))
        if row_data:
            table_data.append(row_data)
    
    return table_data if len(table_data) > 1 else None

def is_efka_data_line(line):
    """
    Î•Î»Î­Î³Ï‡ÎµÎ¹ Î±Î½ Î¼Î¹Î± Î³ÏÎ±Î¼Î¼Î® Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Î´ÎµÎ´Î¿Î¼Î­Î½Î± e-EFKA Ï€Î¯Î½Î±ÎºÎ±
    """
    # Patterns Î³Î¹Î± e-EFKA Î´ÎµÎ´Î¿Î¼Î­Î½Î±
    date_pattern = r'\d{1,2}/\d{1,2}/\d{4}'  # Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚
    amount_pattern = r'[\d,]+\.\d{2}'  # Î Î¿ÏƒÎ¬ Î¼Îµ Î´ÎµÎºÎ±Î´Î¹ÎºÎ¬
    days_pattern = r'^\d+$'  # ÎœÏŒÎ½Î¿ Î±ÏÎ¹Î¸Î¼Î¿Î¯ (Î·Î¼Î­ÏÎµÏ‚)
    
    # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î· Î³ÏÎ±Î¼Î¼Î® Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 2 Î±Ï€ÏŒ Ï„Î± Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰:
    patterns_found = 0
    
    if re.search(date_pattern, line):
        patterns_found += 1
    if re.search(amount_pattern, line):
        patterns_found += 1
    if re.search(days_pattern, line.split()[-1] if line.split() else ''):
        patterns_found += 1
    
    # Î•Ï€Î¯ÏƒÎ·Ï‚ ÎµÎ»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î­Ï‡ÎµÎ¹ Î±ÏÎºÎµÏ„Î­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ (Ï€Î¹Î¸Î±Î½Î¬ ÏƒÏ„Î®Î»ÎµÏ‚)
    words = line.split()
    if len(words) >= 4:
        patterns_found += 1
    
    return patterns_found >= 2

def parse_efka_data_line(line, expected_columns):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î¼Î¹Î± Î³ÏÎ±Î¼Î¼Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ ÏƒÏ„Î®Î»ÎµÏ‚
    """
    # Î§Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î· Î³ÏÎ±Î¼Î¼Î® ÏƒÎµ Î»Î­Î¾ÎµÎ¹Ï‚
    words = line.split()
    
    if len(words) < 4:
        return None
    
    # Î ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¹Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î¼Îµ Î­Î¾Ï…Ï€Î½Î¿ Ï„ÏÏŒÏ€Î¿
    row_data = []
    
    # Î£Ï„Î®Î»Î· 1-2: Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ (Î‘Ï€ÏŒ, ÎˆÏ‰Ï‚)
    date_pattern = r'\d{1,2}/\d{1,2}/\d{4}'
    dates = re.findall(date_pattern, line)
    
    if len(dates) >= 2:
        row_data.extend(dates[:2])
    elif len(dates) == 1:
        row_data.extend([dates[0], ''])
    else:
        row_data.extend(['', ''])
    
    # Î£Ï„Î®Î»Î· 3: Î—Î¼Î­ÏÎµÏ‚ (Î±ÏÎ¹Î¸Î¼ÏŒÏ‚)
    days_pattern = r'^\d+$'
    days = None
    for word in words:
        if re.match(days_pattern, word):
            days = word
            break
    
    row_data.append(days if days else '')
    
    # Î£Ï„Î®Î»ÎµÏ‚ 4-5: Î Î¿ÏƒÎ¬ (ÎœÎ¹ÎºÏ„Î­Ï‚ Î±Ï€Î¿Î´Î¿Ï‡Î­Ï‚, Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î•Î¹ÏƒÏ†Î¿ÏÎ­Ï‚)
    amount_pattern = r'[\d,]+\.\d{2}'
    amounts = re.findall(amount_pattern, line)
    
    if len(amounts) >= 2:
        row_data.extend(amounts[:2])
    elif len(amounts) == 1:
        row_data.extend([amounts[0], ''])
    else:
        row_data.extend(['', ''])
    
    # Î£Ï„Î®Î»Î· 6: Î‘/Îœ Î•ÏÎ³Î¿Î´ÏŒÏ„Î· (ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚)
    am_pattern = r'\d{6,}'  # Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î¼Îµ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 6 ÏˆÎ·Ï†Î¯Î±
    am_match = re.search(am_pattern, line)
    row_data.append(am_match.group() if am_match else '')
    
    # Î£Ï…Î¼Ï€Î»Î·ÏÏÎ½Î¿Ï…Î¼Îµ Î¼Îµ ÎºÎµÎ½Î¬ Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
    while len(row_data) < expected_columns:
        row_data.append('')
    
    return row_data[:expected_columns]

def extract_efka_data_specialized(uploaded_file):
    """
    Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· ÎµÎ¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± e-EFKA PDF Î±ÏÏ‡ÎµÎ¯Î±
    """
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    try:
        st.markdown("### ğŸ” Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î‘Î½Î¬Î»Ï…ÏƒÎ· e-EFKA PDF")
        
        # Î•Î¾Î¬Î³Î¿Ï…Î¼Îµ Ï€Î¯Î½Î±ÎºÎµÏ‚ Î¼Îµ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î¼Î­Î¸Î¿Î´Î¿
        all_tables = extract_efka_tables_specialized(tmp_path)
        
        if not all_tables:
            st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€Î¯Î½Î±ÎºÎµÏ‚ ÏƒÏ„Î¿ PDF Î±ÏÏ‡ÎµÎ¯Î¿")
            return pd.DataFrame()
        
        # Î£Ï…Î½Î´Ï…Î¬Î¶Î¿Ï…Î¼Îµ ÏŒÎ»Î± Ï„Î± DataFrames
        with st.spinner("Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½..."):
            combined_df = pd.concat(all_tables, ignore_index=True)
        
        st.success(f"ğŸ‰ Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÎµÎ¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ {len(combined_df)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ {len(all_tables)} Ï€Î¯Î½Î±ÎºÎµÏ‚")
        return combined_df
    
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î®: {str(e)}")
        return pd.DataFrame()
    
    finally:
        # Î”Î¹Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ Ï„Î¿ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

def main():
    """ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï„Î·Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚"""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ“„ e-EFKA PDF Data Extractor (Specialized)</h1>', unsafe_allow_html=True)
    
    # Sidebar Î¼Îµ Î¿Î´Î·Î³Î¯ÎµÏ‚
    with st.sidebar:
        st.markdown("## ğŸ“‹ ÎŸÎ´Î·Î³Î¯ÎµÏ‚")
        st.markdown("""
        1. **Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ PDF Î±ÏÏ‡ÎµÎ¯Î¿** e-EFKA
        2. **Î Î±Ï„Î®ÏƒÏ„Îµ "Î•Î¾Î±Î³Ï‰Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"**
        3. **Î ÏÎ¿Î²Î¬Î»ÎµÏ„Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±** ÏƒÏ„Î¿Î½ Ï€Î¯Î½Î±ÎºÎ±
        4. **ÎšÎ±Ï„ÎµÎ²Î¬ÏƒÏ„Îµ Ï„Î¿ Excel** Î±ÏÏ‡ÎµÎ¯Î¿
        """)
        
        st.markdown("## ğŸ”§ Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· ÎœÎ­Î¸Î¿Î´Î¿Ï‚")
        st.markdown("""
        **Î‘Ï…Ï„Î® Î· Î­ÎºÎ´Î¿ÏƒÎ·:**
        - **Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î·** Î³Î¹Î± e-EFKA PDF
        - **Custom parsing** Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚
        - **Regex patterns** Î³Î¹Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
        - **Intelligent header detection**
        - **Smart column parsing**
        """)
        
        st.markdown("## âš ï¸ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ¬")
        st.markdown("""
        - Î•Î¾Î¬Î³ÎµÎ¹ Î±Ï€ÏŒ **ÏƒÎµÎ»Î¯Î´Î± 2** ÎºÎ±Î¹ Î¼ÎµÏ„Î¬
        - Î¨Î¬Ï‡Î½ÎµÎ¹ Î³Î¹Î± **Î³Î½Ï‰ÏƒÏ„Î¿ÏÏ‚ headers** e-EFKA
        - **Custom parsing** Î³Î¹Î± ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î®
        - **Pattern matching** Î³Î¹Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
        """)
    
    # ÎšÏÏÎ¹Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“¤ Î‘Î½Î­Î²Î±ÏƒÎ¼Î± PDF Î‘ÏÏ‡ÎµÎ¯Î¿Ï…")
        
        # File uploader
        uploaded_file = st.file_uploader(
            "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ PDF Î±ÏÏ‡ÎµÎ¯Î¿ e-EFKA",
            type=['pdf'],
            help="Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ Ï„Î¿ PDF Î±ÏÏ‡ÎµÎ¯Î¿ Ï€Î¿Ï… Î¸Î­Î»ÎµÏ„Îµ Î½Î± Î±Î½Î±Î»ÏÏƒÎµÏ„Îµ"
        )
        
        if uploaded_file is not None:
            st.success(f"âœ… Î‘Î½ÎµÎ²Î»Î®Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿: {uploaded_file.name}")
            st.info(f"ğŸ“Š ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {uploaded_file.size:,} bytes")
    
    with col2:
        st.markdown("### âš™ï¸ Î•Î½Î­ÏÎ³ÎµÎ¹ÎµÏ‚")
        
        if uploaded_file is not None:
            if st.button("ğŸš€ Î•Î¾Î±Î³Ï‰Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½", type="primary", use_container_width=True):
                # Î•Î¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
                df = extract_efka_data_specialized(uploaded_file)
                
                if not df.empty:
                    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ session state
                    st.session_state['extracted_data'] = df
                    st.session_state['filename'] = uploaded_file.name
                    st.rerun()
        else:
            st.info("Î‘Î½ÎµÎ²Î¬ÏƒÏ„Îµ Ï€ÏÏÏ„Î± Î­Î½Î± PDF Î±ÏÏ‡ÎµÎ¯Î¿")
    
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    if 'extracted_data' in st.session_state and not st.session_state['extracted_data'].empty:
        st.markdown("---")
        st.markdown("### ğŸ“Š Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î•Î¾Î±Î³Ï‰Î³Î®Ï‚")
        
        df = st.session_state['extracted_data']
        
        # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î“ÏÎ±Î¼Î¼Î­Ï‚", len(df))
        with col2:
            st.metric("Î£Ï„Î®Î»ÎµÏ‚", len(df.columns))
        with col3:
            st.metric("Î£ÎµÎ»Î¯Î´ÎµÏ‚", df['Î£ÎµÎ»Î¯Î´Î±'].nunique() if 'Î£ÎµÎ»Î¯Î´Î±' in df.columns else 0)
        with col4:
            st.metric("ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ DataFrame", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
        
        # Î ÏÎ¿Î²Î¿Î»Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        st.markdown("#### ğŸ“‹ Î ÏÎ¿Î²Î¿Î»Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
        
        # Î¦Î¯Î»Ï„ÏÎ±
        if 'Î£ÎµÎ»Î¯Î´Î±' in df.columns:
            pages = sorted(df['Î£ÎµÎ»Î¯Î´Î±'].unique())
            selected_pages = st.multiselect(
                "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ ÏƒÎµÎ»Î¯Î´ÎµÏ‚ Î³Î¹Î± Ï€ÏÎ¿Î²Î¿Î»Î®:",
                options=pages,
                default=pages[:3] if len(pages) > 3 else pages
            )
            
            if selected_pages:
                filtered_df = df[df['Î£ÎµÎ»Î¯Î´Î±'].isin(selected_pages)]
            else:
                filtered_df = df
        else:
            filtered_df = df
        
        # Î Î»Î®Î¸Î¿Ï‚ Î³ÏÎ±Î¼Î¼ÏÎ½ Ï€ÏÎ¿Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
        rows_to_show = st.slider("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î³ÏÎ±Î¼Î¼ÏÎ½ Ï€ÏÎ¿Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·:", 10, min(100, len(filtered_df)), 20)
        
        # Î Î¯Î½Î±ÎºÎ±Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        st.dataframe(
            filtered_df.head(rows_to_show),
            use_container_width=True,
            height=400
        )
        
        # Download button
        st.markdown("#### ğŸ’¾ ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½")
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Excel Î±ÏÏ‡ÎµÎ¯Î¿Ï…
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='e-EFKA_Data', index=False)
        
        output.seek(0)
        
        # Download button
        st.download_button(
            label="ğŸ“¥ ÎšÎ±Ï„ÎµÎ²Î¬ÏƒÏ„Îµ Excel Î±ÏÏ‡ÎµÎ¯Î¿",
            data=output.getvalue(),
            file_name=f"efka_data_{st.session_state.get('filename', 'extracted')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        # Clear button
        if st.button("ğŸ—‘ï¸ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½", use_container_width=True):
            if 'extracted_data' in st.session_state:
                del st.session_state['extracted_data']
            if 'filename' in st.session_state:
                del st.session_state['filename']
            st.rerun()

if __name__ == "__main__":
    main()


